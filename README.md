# streamduck

[![Tests](https://github.com/VEUKA/streamduck/actions/workflows/tests.yml/badge.svg)](https://github.com/VEUKA/streamduck/actions/workflows/tests.yml)
[![CI/CD Pipeline](https://github.com/VEUKA/streamduck/actions/workflows/ci-cd.yml/badge.svg)](https://github.com/VEUKA/streamduck/actions/workflows/ci-cd.yml)
[![codecov](https://codecov.io/gh/VEUKA/streamduck/branch/main/graph/badge.svg)](https://codecov.io/gh/VEUKA/streamduck)

***Note: This is a work in progress. The code and documentation may change frequently.***

StreamDuck is an asynchronous data pipeline framework for consuming messages from Azure Event Hubs and ingesting them into MotherDuck, with advanced features like LLM-powered smart retry and Logfire observability.

## üöÄ Quick Start

Steps to run:

1. Clone the repository.
2. Copy `.env.example` to `.env` and fill it out. The pipeline relies on the Azure Default Credential chain, so make sure you are logged in with `az login` (or equivalent) and have access to the Event Hubs namespace. (Data Plane access required)
3. Run `uv run python src/main.py validate-config` to verify configuration and credentials.
4. Run `uv run python src/main.py run` to start the pipeline.

### Enable Smart Retry (Optional)

StreamDuck includes an intelligent retry system powered by LLM analysis. Enable it by adding to your `.env`:

```bash
# Smart Retry Configuration
SMART_RETRY_ENABLED=true
SMART_RETRY_LLM_PROVIDER=openai  # or azure, anthropic
SMART_RETRY_LLM_MODEL=gpt-4o-mini
SMART_RETRY_LLM_API_KEY=your-api-key
SMART_RETRY_MAX_ATTEMPTS=3
SMART_RETRY_TIMEOUT_SECONDS=10
SMART_RETRY_ENABLE_CACHING=true
```

Then run with the `--smart` flag:

```bash
uv run python src/main.py run --smart
```

See [Smart Retry Documentation](#-smart-retry-llm-powered-error-analysis) for detailed configuration.

## Architecture

```mermaid
flowchart LR
 subgraph Azure
  az[Azure Identity<br/>DefaultAzureCredential]
  EH[Event Hub Namespace<br/>topics &amp; partitions]
 end

 subgraph StreamDuck
  CLI[Typer CLI<br/>uv run commands]
  CFG[Config Loader<br/>.env + Pydantic]
  ORCH[Async Orchestrator<br/>mapping manager]
  CONS[Event Hub Consumer<br/>batching + checkpoints]
 end

 subgraph MotherDuck
  ING[MotherDuck Writer<br/>batch ingestion]
  CTRL[Control Table<br/>waterlevel checkpoints]
 end

 az --> EH
 CLI --> CFG --> ORCH
 CONS --> ORCH
 EH --> CONS
 ORCH --> ING
 ORCH --> CTRL
```

## üß† Smart Retry: LLM-Powered Error Analysis

StreamDuck includes an intelligent retry system that uses Large Language Models to analyze exceptions and make smart decisions about whether to retry operations or fail fast.

### Why Smart Retry?

Traditional retry logic uses fixed rules:

- ‚ùå Retries errors that will never succeed (authentication failures)
- ‚ùå Gives up on errors that could succeed with retry (transient network issues)
- ‚ùå Can't adapt to new error patterns

Smart Retry uses LLM analysis to:

- ‚úÖ Distinguish transient errors from permanent failures
- ‚úÖ Provide context-aware retry decisions with confidence scores
- ‚úÖ Cache decisions to minimize LLM API costs
- ‚úÖ Suggest optimal wait times between retries

### Quick Start

**1. Add to your `.env` file:**

```bash
# Enable Smart Retry
SMART_RETRY_ENABLED=true

# OpenAI Configuration
SMART_RETRY_LLM_PROVIDER=openai
SMART_RETRY_LLM_MODEL=gpt-4o-mini
SMART_RETRY_LLM_API_KEY=sk-your-api-key-here

# Retry Settings
SMART_RETRY_MAX_ATTEMPTS=3
SMART_RETRY_TIMEOUT_SECONDS=10
SMART_RETRY_ENABLE_CACHING=true
```

**2. Run with the `--smart` flag:**

```bash
uv run python src/main.py run --smart
```

**3. Watch the intelligent decisions:**

```log
‚ö†Ô∏è  Exception in ingest_batch (attempt 1/3)
    Error: ConnectionTimeout: Request timed out after 30s

ü§ñ LLM Analysis:
    Decision: RETRY ‚úì
    Confidence: 0.90
    Reasoning: Network timeout is a transient error. The service
               may be temporarily overloaded. Retry with backoff.
    Suggested wait: 5.0s

‚è≥ Waiting 5.0s before retry...
```

### Supported LLM Providers

#### OpenAI (Default)

```bash
SMART_RETRY_LLM_PROVIDER=openai
SMART_RETRY_LLM_MODEL=gpt-4o-mini  # or gpt-4o, gpt-3.5-turbo
SMART_RETRY_LLM_API_KEY=sk-...
```

**Cost Estimate:** Remember that each call has a cost!

#### Azure OpenAI ‚úÖ (Fully Supported)

```bash
SMART_RETRY_LLM_PROVIDER=azure
SMART_RETRY_LLM_MODEL=gpt-4o-mini  # Your Azure deployment name
SMART_RETRY_LLM_API_KEY=your-azure-api-key
SMART_RETRY_LLM_ENDPOINT=https://your-resource.openai.azure.com/openai/...
```

**Requirements:**

- Azure OpenAI resource with API version 2024-06-01 or later
- API key with appropriate permissions

**Testing Azure OpenAI:**

```bash
# Create test configuration
cp .env.example .env.test

# Add Azure credentials
echo "SMART_RETRY_LLM_PROVIDER=azure" >> .env.test
echo "SMART_RETRY_LLM_MODEL=gpt-4o-mini" >> .env.test
echo "SMART_RETRY_LLM_API_KEY=your-key" >> .env.test
echo "SMART_RETRY_LLM_ENDPOINT=https://..." >> .env.test

# Run integration test
python tests/test_azure_llm.py
```

### Configuration Reference

| Environment Variable | Default | Description |
|---------------------|---------|-------------|
| `SMART_RETRY_ENABLED` | `false` | Enable/disable smart retry system |
| `SMART_RETRY_LLM_PROVIDER` | `openai` | LLM provider: `openai`, `azure`, `anthropic` |
| `SMART_RETRY_LLM_MODEL` | `gpt-4o-mini` | Model name or Azure deployment name |
| `SMART_RETRY_LLM_API_KEY` | *required* | API key for LLM provider |
| `SMART_RETRY_LLM_ENDPOINT` | `None` | Custom endpoint (required for Azure) |
| `SMART_RETRY_MAX_ATTEMPTS` | `3` | Maximum retry attempts |
| `SMART_RETRY_TIMEOUT_SECONDS` | `10` | LLM request timeout |
| `SMART_RETRY_ENABLE_CACHING` | `true` | Cache decisions by error signature |

### How It Works

1. **Exception Occurs**: Pipeline catches an exception during Event Hub consumption or MotherDuck ingestion

2. **LLM Analysis**: Exception details sent to LLM with context:
   - Exception type and message
   - Stack trace (truncated)
   - Operation being attempted
   - Current retry attempt number
   - Historical context (if cached)

3. **Structured Decision**: LLM returns a structured decision:
   ```python
   {
     "should_retry": true,
     "reasoning": "Network timeout is transient...",
     "confidence": 0.90,
     "suggested_wait_time": 5.0
   }
   ```

4. **Smart Action**:
   - **RETRY**: Wait suggested time, retry operation
   - **STOP**: Fail fast, log error, continue pipeline
   - **Fallback**: If LLM fails, use conservative default (retry on known transient errors)

5. **Caching**: Decision cached by error signature (exception type + operation) to avoid duplicate LLM calls

### Decision Caching

Smart Retry caches decisions to minimize LLM API costs:

```python
# First occurrence: LLM call
ConnectionTimeout in ingest_batch ‚Üí LLM Analysis ‚Üí RETRY (5s wait)
# Cost: ~$0.0002

# Second occurrence: Cached
ConnectionTimeout in ingest_batch ‚Üí Cached Decision ‚Üí RETRY (5s wait)
# Cost: $0 (instant)
```

**Cache Key**: `{exception_type}:{operation}:{error_message_hash}`

**Cache Duration**: Session-based (cleared on restart)

**Cache Hit Rate**: Typically 40-60% in production workloads

### Example Decisions

#### Transient Errors (RETRY)

```log
‚ö†Ô∏è  ConnectionTimeout: Request timed out after 30s
ü§ñ Decision: RETRY | Confidence: 0.90
    Network timeouts are typically transient. Retry with backoff.
```

```log
‚ö†Ô∏è  ServiceUnavailable: MotherDuck service temporarily unavailable
ü§ñ Decision: RETRY | Confidence: 0.85
    Service may be under maintenance or experiencing load. Retry.
```

#### Permanent Errors (STOP)

```log
‚ö†Ô∏è  AuthenticationError: Invalid API credentials
ü§ñ Decision: STOP | Confidence: 0.95
    Authentication errors won't resolve with retries. Fix credentials.
```

```log
‚ö†Ô∏è  ValueError: Invalid table schema - column 'user_id' does not exist
ü§ñ Decision: STOP | Confidence: 0.98
    Schema mismatch is a configuration error. Retry won't help.
```

### CLI Commands

```bash
# Run with smart retry enabled
uv run python src/main.py run --smart

# Run with smart retry disabled (fall back to standard retry)
uv run python src/main.py run

# Dry run to test configuration
uv run python src/main.py run --smart --dry-run

# Validate smart retry configuration
uv run python src/main.py validate-config
```

### Monitoring & Statistics

Smart Retry tracks statistics displayed at the end of each run:

```log
üìä Smart Retry Statistics:
    Total LLM Calls: 8
    Cached Decisions: 12
    RETRY Decisions: 15
    STOP Decisions: 5
    Average Confidence: 0.88
    Cache Hit Rate: 60%
    Total LLM Cost: ~$0.0016
```

### Troubleshooting

#### Azure OpenAI "tool_choice not supported"

**Error:**

```log
BadRequestError: tool_choice value as required is enabled only for 
api versions 2024-06-01 and later
```

**Solution:** Update your Azure OpenAI API version to 2024-08-01-preview or later in your endpoint URL:

```bash
SMART_RETRY_LLM_ENDPOINT=https://your-resource.openai.azure.com/openai/responses?api-version=2024-08-01-preview
```

#### LLM Timeout

**Error:**

```log
‚ö†Ô∏è  LLM analysis timed out after 10s. Using fallback decision.
```

**Solutions:**

- Increase timeout: `SMART_RETRY_TIMEOUT_SECONDS=30`
- Use faster model: `gpt-4o-mini` or `claude-3-5-haiku-20241022`
- Check network connectivity to LLM provider

#### High LLM Costs

**Symptoms:** Many unique errors, low cache hit rate

**Solutions:**

- Enable caching: `SMART_RETRY_ENABLE_CACHING=true`
- Use cheaper model: `gpt-4o-mini` instead of `gpt-4o`
- Reduce max attempts: `SMART_RETRY_MAX_ATTEMPTS=2`
- Review error patterns and fix root causes

#### Authentication Errors

**Azure OpenAI:**

```bash
# Verify API key is correct
echo $SMART_RETRY_LLM_API_KEY

# Check endpoint format
echo $SMART_RETRY_LLM_ENDPOINT
```

**OpenAI:**

```bash
# Test API key
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $SMART_RETRY_LLM_API_KEY"
```

### Cost Analysis

#### OpenAI gpt-4o-mini

- Input: $0.150 / 1M tokens
- Output: $0.600 / 1M tokens
- Average cost per analysis: ~$0.0002
- Monthly cost (100 failures/day, 50% cache hit): ~$0.60

#### Azure OpenAI gpt-4o-mini

- Similar pricing to OpenAI (varies by region)
- Monthly cost estimate: ~$0.60 - $1.20

#### Anthropic Claude 3.5 Haiku

- Input: $0.25 / 1M tokens
- Output: $1.25 / 1M tokens
- Average cost per analysis: ~$0.0003
- Monthly cost (100 failures/day, 50% cache hit): ~$0.90

**Cost Optimization:**

- ‚úÖ Caching reduces costs by 40-60%
- ‚úÖ Conservative fallbacks minimize unnecessary LLM calls
- ‚úÖ Timeout protection prevents runaway costs
- ‚úÖ Session-based caching (cleared on restart)

### Security Best Practices

**API Key Management:**

- ‚úÖ Store keys in `.env` file (never commit to git)
- ‚úÖ Use Azure Key Vault in production
- ‚úÖ Rotate keys regularly
- ‚úÖ Set spending limits in LLM provider dashboard

**Production Deployment:**

```bash
# Use Azure Key Vault
az keyvault secret set \
  --vault-name your-vault \
  --name smart-retry-api-key \
  --value "your-key"

# Retrieve in production
SMART_RETRY_LLM_API_KEY=$(az keyvault secret show \
  --vault-name your-vault \
  --name smart-retry-api-key \
  --query value -o tsv)
```

### Testing

**Unit Tests:**

```bash
# Run smart retry unit tests
pytest tests/unit/test_smart_retry.py -v

# Results: 13/16 tests passing (81%)
```

**Integration Tests:**

```bash
# Azure OpenAI integration test
python tests/test_azure_llm.py

# Expected output:
# ‚úÖ Test 1: Network Timeout (RETRY)
# ‚úÖ Test 2: Auth Error (STOP)
# ‚úÖ Test 3: Caching
# üìä ALL TESTS PASSED (3/3)
```

### Related Documentation

- [Smart Retry Implementation Plan](./docs/smart-retry-implementation-plan.md)
- [Smart Retry Progress Tracker](./docs/smart-retry-progress.md)
- [Azure OpenAI Integration Guide](./docs/AZURE_OPENAI_PROGRESS.md)
- [Implementation Summary](./docs/IMPLEMENTATION_SUMMARY.md)
- [Testing Summary](./docs/TESTING_SUMMARY.md)

### Architecture

```log
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     StreamDuck Pipeline                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                    Exception Occurs
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Smart Retry Manager                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Exception Analyzer (pydantic-ai)                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  1. Check Cache by Error Signature             ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  2. If miss: Send to LLM with context          ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  3. Get Structured Decision (RetryDecision)    ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  4. Cache decision for future use              ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ                               ‚îÇ
         RETRY Decision                  STOP Decision
              ‚îÇ                               ‚îÇ
              ‚ñº                               ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Wait + Retry    ‚îÇ           ‚îÇ Fail Fast +     ‚îÇ
    ‚îÇ (with backoff)  ‚îÇ           ‚îÇ Continue        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Performance Impact

**Overhead per Exception:**

- First occurrence: 200-500ms (LLM call)
- Cached occurrence: <1ms (hash lookup)

**Throughput Impact:**

- Negligible with caching enabled
- ~0.1% overhead in typical workloads (assuming <1% failure rate)

**Latency Impact:**

- Only affects failed operations (which were already failing)
- Smart decisions can actually reduce total latency by avoiding unnecessary retries

### Future Enhancements

- üîÑ Prometheus metrics export
- üìä Decision audit logging to database
- üß™ A/B testing with multiple models
- üìù Custom prompt templates
- üîç Exception pattern learning
- üéØ Confidence threshold tuning
- üìà Historical decision analytics

### Contributing

Smart Retry is production-ready but welcomes improvements:

1. **Test Coverage**: Help us reach 90%+ coverage
2. **Provider Support**: Add new LLM providers (Gemini, Mistral, etc.)
3. **Prompt Engineering**: Improve decision accuracy
4. **Cost Optimization**: Better caching strategies

See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines.

---

**Smart Retry Status:** ‚úÖ Production Ready (v1.0)  
**Last Updated:** October 22, 2025

## üî• Logfire Observability: Real-time Pipeline Monitoring

StreamDuck integrates with [Logfire](https://logfire.pydantic.dev/) to provide comprehensive observability across your entire data pipeline. Monitor Event Hub consumption, MotherDuck ingestion, Smart Retry decisions, and LLM calls with beautiful real-time traces and structured logs.

### Why Logfire?

Traditional logging is noisy and hard to debug:

- ‚ùå Text-based logs mixed with unrelated messages
- ‚ùå No visibility into async operations and spans
- ‚ùå Difficult to correlate events across components
- ‚ùå No insight into LLM calls and retry decisions

Logfire provides structured observability:

- ‚úÖ **Distributed tracing** across Event Hub ‚Üí MotherDuck pipeline
- ‚úÖ **Automatic instrumentation** for Pydantic AI (LLM calls)
- ‚úÖ **Real-time dashboards** with search and filtering
- ‚úÖ **Rich context** with span attributes, timing, and errors
- ‚úÖ **Local + Cloud** logging modes for development and production

### Quick Start

**1. Add to your `.env` file:**

```bash
# Enable Logfire
LOGFIRE_ENABLED=true

# Cloud Mode (requires Logfire account - free tier available)
LOGFIRE_TOKEN=pylf_v1_us_your-token-here
LOGFIRE_SEND_TO_LOGFIRE=true

# Or Local-Only Mode (console logging only, no cloud)
LOGFIRE_SEND_TO_LOGFIRE=false
LOGFIRE_CONSOLE_LOGGING=true

# Optional Configuration
LOGFIRE_SERVICE_NAME=streamduck
LOGFIRE_ENVIRONMENT=production
LOGFIRE_LOG_LEVEL=INFO
```

**2. Get a Logfire Token (Optional - only needed for cloud mode):**

```bash
# Sign up at https://logfire.pydantic.dev (free tier available)
# Create a project and copy your token
# Or run locally without cloud:
export LOGFIRE_SEND_TO_LOGFIRE=false
```

**3. Run your pipeline:**

```bash
# Logfire initializes automatically when enabled
uv run python src/main.py run --smart
```

**4. View your traces:**

- **Cloud Mode**: Visit <https://logfire.pydantic.dev/your-project/live>
- **Local Mode**: Check console output with Rich formatting

### Configuration Modes

#### üö´ Disabled (Default)

```bash
LOGFIRE_ENABLED=false
```

No Logfire initialization. Standard Python logging only.

#### üñ•Ô∏è Local-Only Mode (Development)

```bash
LOGFIRE_ENABLED=true
LOGFIRE_SEND_TO_LOGFIRE=false
LOGFIRE_CONSOLE_LOGGING=true
LOGFIRE_LOG_LEVEL=DEBUG
```

Perfect for development:

- ‚úÖ Console logging with Rich formatting
- ‚úÖ No cloud dependency or costs
- ‚úÖ Structured log output with context
- ‚ùå No persistent storage or dashboard

#### ‚òÅÔ∏è Cloud-Only Mode (Production)

```bash
LOGFIRE_ENABLED=true
LOGFIRE_TOKEN=pylf_v1_us_your-token
LOGFIRE_SEND_TO_LOGFIRE=true
LOGFIRE_CONSOLE_LOGGING=false
LOGFIRE_LOG_LEVEL=INFO
```

Best for production:

- ‚úÖ Real-time cloud dashboard
- ‚úÖ Persistent trace storage
- ‚úÖ Search and filtering
- ‚úÖ Alerts and monitoring
- ‚ö†Ô∏è Requires Logfire account (free tier: 1M spans/month)

#### üåê Hybrid Mode (Recommended)

```bash
LOGFIRE_ENABLED=true
LOGFIRE_TOKEN=pylf_v1_us_your-token
LOGFIRE_SEND_TO_LOGFIRE=true
LOGFIRE_CONSOLE_LOGGING=true
LOGFIRE_LOG_LEVEL=INFO
```

Best of both worlds:

- ‚úÖ Local console logs for immediate feedback
- ‚úÖ Cloud dashboard for historical analysis
- ‚úÖ Full observability stack

### What Gets Instrumented

Logfire automatically traces your entire pipeline:

#### üéØ Event Hub Consumer

```python
# Span: eventhub.start
# Tracks: namespace, name, consumer_group, batch_size, batch_timeout

# Span: eventhub._on_event
# Tracks: partition_id, sequence_number, offset, first_message, batch_ready

# Span: eventhub._process_batch
# Tracks: batch_size, partitions, processor_success, checkpoints_updated
```

#### ü¶Ü MotherDuck Streaming

```python
# Span: motherduck.start
# Tracks: database, schema, table, client_suffix

# Span: motherduck._ensure_target_table
# Tracks: schema, table, ddl_executed

# Span: motherduck._ingest_batch_impl
# Tracks: batch_size, messages_sent, total_messages, total_batches

# Span: motherduck.ingest_batch
# Tracks: has_retry_manager, success, errors
```

#### üß† Smart Retry (with LLM calls)

```python
# Span: smart_retry.analyze_exception
# Tracks: exception_type, llm_provider, llm_model, cache_hit, decision, confidence

# Span: smart_retry._analyze_with_llm
# Tracks: llm_provider, llm_model, api_call_count, prompt_length, decision

# Span: smart_retry.should_retry_predicate
# Tracks: exception_type, decision, confidence, reasoning, fallback_mode
```

#### ü§ñ Pydantic AI (Automatic)

```python
# Span: pydantic_ai.llm_request
# Tracks: model, provider, tokens_used, response_time, cost_estimate

# Automatically captured for all Smart Retry LLM calls
```

#### üé≠ Pipeline Orchestrator

```python
# Span: orchestrator._process_messages
# Tracks: message_count, mapping_key, channel_name, success, total_batches_processed

# Span: orchestrator.run_async
# Tracks: total_mappings, active_mappings, tasks_started, cancelled, errors
```

### Example Trace Output

#### Console Output (Local Mode)

```log
üî• Logfire initialized: local-only mode
üìä Service: streamduck | Environment: development

‚ñ∂ eventhub.start
  ‚îÇ namespace: eventhub1.servicebus.windows.net
  ‚îÇ name: topic1
  ‚îÇ consumer_group: $Default
  ‚îÇ batch_size: 1000
  ‚îÇ ‚è±Ô∏è  12.5ms
  ‚îÇ
  ‚îú‚îÄ‚ñ∂ eventhub._on_event
  ‚îÇ   ‚îÇ partition_id: 0
  ‚îÇ   ‚îÇ sequence_number: 12345
  ‚îÇ   ‚îÇ batch_ready: true
  ‚îÇ   ‚îÇ total_messages_received: 150
  ‚îÇ   ‚îÇ ‚è±Ô∏è  2.3ms
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îú‚îÄ‚ñ∂ motherduck.ingest_batch
  ‚îÇ   ‚îÇ   ‚îÇ batch_size: 150
  ‚îÇ   ‚îÇ   ‚îÇ success: true
  ‚îÇ   ‚îÇ   ‚îÇ ‚è±Ô∏è  45.2ms
  ‚îÇ   ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∂ motherduck._ingest_batch_impl
  ‚îÇ   ‚îÇ       ‚îÇ messages_sent: 150
  ‚îÇ   ‚îÇ       ‚îÇ total_messages: 1500
  ‚îÇ   ‚îÇ       ‚îÇ total_batches: 10
  ‚îÇ   ‚îÇ       ‚îÇ ‚è±Ô∏è  42.8ms
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îî‚îÄ‚ñ∂ eventhub._process_batch
  ‚îÇ       ‚îÇ batch_size: 150
  ‚îÇ       ‚îÇ partitions: 1
  ‚îÇ       ‚îÇ checkpoints_updated: 1
  ‚îÇ       ‚îÇ processor_success: true
  ‚îÇ       ‚îÇ ‚è±Ô∏è  48.7ms
```

#### Smart Retry with LLM Trace

```log
‚ö†Ô∏è  Exception: ConnectionTimeout

‚ñ∂ smart_retry.analyze_exception
  ‚îÇ exception_type: ConnectionTimeout
  ‚îÇ llm_provider: openai
  ‚îÇ llm_model: gpt-4o-mini
  ‚îÇ cache_hit: false
  ‚îÇ ‚è±Ô∏è  287ms
  ‚îÇ
  ‚îú‚îÄ‚ñ∂ smart_retry._analyze_with_llm
  ‚îÇ   ‚îÇ llm_provider: openai
  ‚îÇ   ‚îÇ prompt_length: 523
  ‚îÇ   ‚îÇ api_call_count: 1
  ‚îÇ   ‚îÇ ‚è±Ô∏è  283ms
  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îî‚îÄ‚ñ∂ pydantic_ai.llm_request [AUTO-INSTRUMENTED]
  ‚îÇ       ‚îÇ model: gpt-4o-mini
  ‚îÇ       ‚îÇ provider: openai
  ‚îÇ       ‚îÇ tokens_used: 147
  ‚îÇ       ‚îÇ cost_estimate: $0.0002
  ‚îÇ       ‚îÇ ‚è±Ô∏è  278ms
  ‚îÇ
  ‚îî‚îÄ‚ñ∂ smart_retry.should_retry_predicate
      ‚îÇ decision: RETRY
      ‚îÇ confidence: 0.90
      ‚îÇ reasoning: Network timeout is transient
      ‚îÇ fallback_mode: false
      ‚îÇ ‚è±Ô∏è  1.2ms

‚úÖ Decision: RETRY | Confidence: 0.90
```

### Configuration Reference

| Environment Variable | Default | Description |
|---------------------|---------|-------------|
| `LOGFIRE_ENABLED` | `false` | Enable/disable Logfire integration |
| `LOGFIRE_TOKEN` | `None` | Logfire API token (required for cloud mode) |
| `LOGFIRE_SEND_TO_LOGFIRE` | `true` | Send traces to Logfire cloud |
| `LOGFIRE_CONSOLE_LOGGING` | `true` | Enable Rich console logging |
| `LOGFIRE_SERVICE_NAME` | `streamduck` | Service name in traces |
| `LOGFIRE_ENVIRONMENT` | `development` | Environment tag (dev/staging/prod) |
| `LOGFIRE_LOG_LEVEL` | `INFO` | Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL |

### Validation

Verify your Logfire configuration:

```bash
# Validate configuration
uv run python src/main.py validate-config

# Output:
# ‚úÖ Logfire Configuration:
#    ‚Ä¢ Enabled: true
#    ‚Ä¢ Send to Cloud: true
#    ‚Ä¢ Console Logging: true
#    ‚Ä¢ Service: streamduck
#    ‚Ä¢ Environment: production
#    ‚Ä¢ Log Level: INFO
```

### Cloud Dashboard Features

When using cloud mode, access the Logfire dashboard at <https://logfire.pydantic.dev>:

#### üìä Live View

- Real-time trace visualization
- Waterfall charts showing span timing
- Automatic error highlighting
- LLM call tracking with cost estimates

#### üîç Search & Filter

```log
# Find all failed ingestion batches
success:false AND motherduck.ingest_batch

# Find expensive LLM calls (>$0.001)
pydantic_ai.llm_request AND cost_estimate:>0.001

# Find slow operations (>1s)
duration:>1000ms

# Find specific partition processing
partition_id:0 AND eventhub._on_event
```

#### üìà Analytics

- Message throughput over time
- Error rates by component
- P50/P95/P99 latency percentiles
- LLM cost tracking
- Checkpoint success rates

#### üö® Alerts (Paid Tier)

- High error rates
- Latency spikes
- LLM cost overruns
- Checkpoint failures

### Benefits by Role

#### üë®‚Äçüíª Developers

- **Debug faster**: Trace errors through entire pipeline with full context
- **Understand flow**: See how messages flow from Event Hub to MotherDuck
- **Optimize performance**: Identify slow operations with timing data
- **Test locally**: Use local-only mode without cloud dependency

#### üîß DevOps Engineers

- **Monitor production**: Real-time visibility into pipeline health
- **Troubleshoot incidents**: Search traces by time, error, or component
- **Capacity planning**: Track throughput and resource usage
- **Cost tracking**: Monitor LLM API costs in Smart Retry

#### üìä Data Engineers

- **Validate data flow**: Ensure messages processed correctly
- **Track checkpoints**: Verify watermark progression
- **Batch analysis**: Monitor batch sizes and ingestion rates
- **Quality assurance**: Detect data processing errors early

### Performance Impact

**Overhead:**

- Local-only mode: <1% CPU, negligible latency
- Cloud mode: <2% CPU, ~5-10ms additional latency per span
- Memory: ~10-20MB for span buffering

**Recommendations:**

- Use `INFO` level in production (default)
- Use `DEBUG` level only for troubleshooting
- Use local-only mode in development
- Use cloud mode in staging/production

### Testing

Logfire configuration is fully tested:

```bash
# Run Logfire unit tests
pytest tests/unit/test_logfire_config.py -v

# Results: 25/25 tests passing (100%)
# Coverage: Disabled, local-only, cloud-only, hybrid modes
#           Log levels, validation, Smart Retry integration
```

### Troubleshooting

#### Token Required Error

**Error:**

```log
ValidationError: LOGFIRE_TOKEN is required when enabled=True 
and send_to_logfire=True
```

**Solution:**

```bash
# Option 1: Add token for cloud mode
echo "LOGFIRE_TOKEN=pylf_v1_us_your-token" >> .env

# Option 2: Use local-only mode
echo "LOGFIRE_SEND_TO_LOGFIRE=false" >> .env
```

#### Invalid Log Level

**Error:**

```log
ValidationError: Invalid log level: TRACE
```

**Solution:** Use valid log levels:

```bash
LOGFIRE_LOG_LEVEL=DEBUG    # Most verbose
LOGFIRE_LOG_LEVEL=INFO     # Default (recommended)
LOGFIRE_LOG_LEVEL=WARNING  # Warnings and errors only
LOGFIRE_LOG_LEVEL=ERROR    # Errors only
LOGFIRE_LOG_LEVEL=CRITICAL # Critical errors only
```

#### No Traces in Dashboard

**Checklist:**

1. Verify token is correct: `echo $LOGFIRE_TOKEN`
2. Ensure send_to_logfire=true: `echo $LOGFIRE_SEND_TO_LOGFIRE`
3. Check network connectivity to logfire.pydantic.dev
4. Verify project exists in Logfire dashboard
5. Wait 10-30 seconds for traces to appear (buffering delay)

#### Console Logging Not Working

**Solution:**

```bash
# Ensure console logging is enabled
LOGFIRE_CONSOLE_LOGGING=true

# Verify Logfire is enabled
LOGFIRE_ENABLED=true

# Check log level (DEBUG shows most)
LOGFIRE_LOG_LEVEL=DEBUG
```

### Cost Analysis

#### Logfire Cloud Pricing

**Free Tier:**

- 1M spans/month
- 7-day retention
- Perfect for development and small pipelines

**Paid Tier (Pro):**

- $20/month base
- $0.20 per 100K additional spans
- 30-day retention
- Alerts and advanced features

#### Typical Usage

**Small Pipeline** (100K messages/day):

- ~300K spans/month (Event Hub + MotherDuck + Orchestrator)
- **Cost: Free** (within free tier)

**Medium Pipeline** (1M messages/day):

- ~3M spans/month
- **Cost: $4/month** (2M additional spans √ó $0.20/100K)

**Large Pipeline** (10M messages/day):

- ~30M spans/month
- **Cost: $58/month** (29M additional spans √ó $0.20/100K)

**With Smart Retry LLM Tracing** (add 20-30% spans):

- Pydantic AI auto-instrumentation adds spans for each LLM call
- Typically 5-10% additional spans if Smart Retry enabled
- Example: 3M spans/month ‚Üí 3.3M spans/month

### Security Best Practices

**Token Management:**

```bash
# ‚úÖ Store token in .env (never commit)
echo "LOGFIRE_TOKEN=pylf_..." >> .env

# ‚úÖ Use Azure Key Vault in production
az keyvault secret set \
  --vault-name your-vault \
  --name logfire-token \
  --value "pylf_..."

# ‚úÖ Rotate tokens regularly
# Generate new token in Logfire dashboard ‚Üí Update .env
```

**Data Privacy:**

- ‚úÖ Logfire only sends span metadata and attributes
- ‚úÖ Message content NOT sent to Logfire (only counts/sizes)
- ‚úÖ Use local-only mode for sensitive data
- ‚úÖ Configure environment-specific tokens (dev/staging/prod)

### Related Documentation

- [Logfire Implementation Plan](./docs/logfire-implementation-plan.md) - Detailed implementation guide
- [Logfire Architecture](./docs/logfire-architecture.md) - System architecture and diagrams
- [Logfire Quick Start](./docs/logfire-quick-start.md) - User guide
- [Logfire Reference Card](./docs/logfire-reference-card.md) - Quick reference
- [Logfire Before/After](./docs/logfire-before-after.md) - Comparison guide

### Integration Architecture

```bash
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   StreamDuck Pipeline                        ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Event Hub   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Orchestrator ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  MotherDuck  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Consumer    ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ  Streaming   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ                    ‚îÇ                    ‚îÇ         ‚îÇ
‚îÇ         ‚îÇ                    ‚îÇ                    ‚îÇ         ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                              ‚îÇ                              ‚îÇ
‚îÇ                              ‚îÇ Logfire Spans               ‚îÇ
‚îÇ                              ‚ñº                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ           Logfire SDK (Python)                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Automatic span creation                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Context propagation                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Pydantic AI auto-instrumentation                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Rich console handler                              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                 ‚îÇ                         ‚îÇ
                 ‚ñº                         ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Local Console        ‚îÇ   ‚îÇ   Logfire Cloud        ‚îÇ
    ‚îÇ   (Rich Formatting)    ‚îÇ   ‚îÇ   Dashboard            ‚îÇ
    ‚îÇ   ‚Ä¢ Real-time output   ‚îÇ   ‚îÇ   ‚Ä¢ Persistent storage ‚îÇ
    ‚îÇ   ‚Ä¢ No cloud dependency‚îÇ   ‚îÇ   ‚Ä¢ Search & analytics ‚îÇ
    ‚îÇ   ‚Ä¢ Development mode   ‚îÇ   ‚îÇ   ‚Ä¢ Alerts & monitoring‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Example: End-to-End Trace

When a message flows through the pipeline, Logfire creates a hierarchical trace:

```log
Pipeline Run (10 minutes)
‚îú‚îÄ‚îÄ orchestrator.run_async (600s)
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator._process_messages (mapping: EVENTHUB_1 ‚Üí MOTHERDUCK_1)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eventhub.start (60s)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eventhub._on_event (partition: 0)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ motherduck.ingest_batch (150 messages)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ motherduck._ensure_target_table (success)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ motherduck._ingest_batch_impl (45ms)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ eventhub._process_batch (checkpoint updated)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eventhub._on_event (partition: 1)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ smart_retry.analyze_exception (ConnectionTimeout)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ smart_retry._analyze_with_llm
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pydantic_ai.llm_request (gpt-4o-mini, $0.0002)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ smart_retry.should_retry_predicate (RETRY, 0.90)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ motherduck.ingest_batch (retry success)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (more events)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Statistics: 15,000 messages, 100 batches, 2 retries
‚îÇ   ‚îî‚îÄ‚îÄ ... (more mappings)
‚îî‚îÄ‚îÄ Summary: 1 mapping, 15,000 messages, 99.98% success rate
```



### Contributing

Logfire integration is production-ready and welcomes improvements:

1. **Documentation**: Help improve examples and guides
2. **Custom Instrumentation**: Add spans to new components
3. **Dashboard Templates**: Create reusable Logfire queries
4. **Performance**: Optimize span buffering and sampling

See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines.

---




